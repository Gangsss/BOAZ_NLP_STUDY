{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01.Skip-gram-Naive-Softmax.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Gangsss/BOAZ_NLP_STUDY/blob/master/01_Skip_gram_Naive_Softmax_ipynb_colab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "rE2YIZca4hfe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Skip-gram with Naive Softmax"
      ]
    },
    {
      "metadata": {
        "id": "OTd65m8P6dfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQiTjguY4hfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "flatten = lambda l : [item for sublist in l for item in sublist]\n",
        "random.seed(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OP-IB4he4hft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a805a566-8b2a-4029-98db-f0ddfaa983c0"
      },
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(nltk.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3.0.post4\n",
            "3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u83w8mAOMsv1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ENV"
      ]
    },
    {
      "metadata": {
        "id": "4fz6rPDl4hf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "#gpus = [0]\n",
        "#torch.cuda.set_device(0)\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sz5zQqoKL6q6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1a2fc88-6ecd-46d2-beff-d042063302bf"
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA  #colab 이용하기"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "iIQJqMNc4hf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "    random.shuffle(train_data)\n",
        "    sindex = 0 # Start Index\n",
        "    eindex = batch_size # End Index\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex:eindex]\n",
        "        sindex  = eindex\n",
        "        eindex = eindex + batch_size\n",
        "        yield batch\n",
        "        \n",
        "    if eindex >= len(train_data):\n",
        "        batch = train_data[sindex:]\n",
        "        yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUyZthj54hgE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] \n",
        "            if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return Variable(LongTensor(idxs))\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "    return Variable(LongTensor([word2index[word]]) \n",
        "            if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pQhO1iR4hgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Load and Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "mBSAGR8P4hgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "bf566b7e-3450-42ba-e0bd-e2f8db4c48ba"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "HzyHvHlD4hgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "1ccef324-158d-44e9-f55c-35bb4d4a9cbb"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "#The sents() function divides the text up into its sentences, where each sentence is a list of words:\n",
        "#nltk.corpus.gutenberg.sents('melville-moby_dick.txt')\n",
        "corpus = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')[:100]\n",
        "#corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100]\n",
        "#print(corpus)\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus] # lower() 소문자 변환\n",
        "print(corpus[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'], ['etymology', '.'], ['(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')'], ['the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.'], ['he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.'], ['he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.'], ['\"', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-', 'fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'h', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.\"'], ['--', 'hackluyt'], ['\"', 'whale', '.'], ['...', 'sw', '.', 'and', 'dan', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ogtS6ZxOOLBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed3b3d5e-1081-41a6-8d36-37852fdedda7"
      },
      "cell_type": "code",
      "source": [
        "print(flatten(corpus)[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0ycQcRC4hgW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract Stopwords from unigram distributions's tails"
      ]
    },
    {
      "metadata": {
        "id": "wLhfCR4D4hgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "c4084a01-6a46-4db0-c738-a09783e2bad1"
      },
      "cell_type": "code",
      "source": [
        "# Word Count\n",
        "word_count = Counter(flatten(corpus))\n",
        "border = int(len(word_count) * 0.01)\n",
        "\n",
        "# Stopwords (가장 자주 나온 words를 stopwords로)\n",
        "# 가장 자주 나온 words + Inversed Matrix에서 가장 자주 나온(?) words\n",
        "stopwords = word_count.most_common()[:border] + reversed(word_count.most_common())[:border]\n",
        "stopwords = [s[0] for s in stopwords]\n",
        "stopwords"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ab7d8d1250bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Stopwords (가장 자주 나온 words를 stopwords로)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 가장 자주 나온 words + Inversed Matrix에서 가장 자주 나온(?) words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list_reverseiterator' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ahgsfYRq4hgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Vocab"
      ]
    },
    {
      "metadata": {
        "id": "SrWf4UI94hgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e38b9d27-a594-45fb-83f1-a78ed29c1786"
      },
      "cell_type": "code",
      "source": [
        "# Corpus에서 Stopwords를 제거\n",
        "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
        "vocab.append('<UNK>')\n",
        "print('Corpus : ', len(set(flatten(corpus))))\n",
        "print('Vocab : ', len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus :  2607\n",
            "Vocab :  2556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1KVSDcvo4hgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2index = {'<UNK>' : 0}\n",
        "\n",
        "# word2index - word : index 형태\n",
        "for word in vocab:\n",
        "    if word2index.get(word) is None:\n",
        "        word2index[word] = len(word2index)\n",
        "\n",
        "# index2word - index : word 형태로 변경\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ojwtyvkd4hgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2ead8318-5cfc-4405-ba26-c792ef0e0fcd"
      },
      "cell_type": "code",
      "source": [
        "# Window Size 기준으로 \n",
        "WINDOW_SIZE = 3\n",
        "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c \n",
        "              + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
        "print(windows[0])\n",
        "print(windows[1])\n",
        "print(windows[2])\n",
        "print(windows[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by')\n",
            "('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman')\n",
            "('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "('be', 'ready', 'directly', '.\"', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uJrujQMH4hgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a6245fee-47fd-4eae-ca41-f2c26b41a3f0"
      },
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "\n",
        "for window in windows:\n",
        "    for i in range(WINDOW_SIZE * 2 + 1):\n",
        "        if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
        "            continue\n",
        "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
        "\n",
        "print(train_data[:WINDOW_SIZE * 2])\n",
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n",
            "('[', 'moby')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YTwwKowT4hgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "outputId": "ce1ade67-1df9-4e83-f1b5-446fb6b7723c"
      },
      "cell_type": "code",
      "source": [
        "X_p = []\n",
        "y_p = []\n",
        "\n",
        "# 위에서 선언한 prepare_word 함수 사용\n",
        "for data in train_data:\n",
        "    X_p.append(prepare_word(data[0], word2index).view(1, -1))\n",
        "    y_p.append(prepare_word(data[1], word2index).view(1, -1))\n",
        "\n",
        "train_data = list(zip(X_p, y_p))\n",
        "\n",
        "\n",
        "print(X_p[0])\n",
        "print(y_p[0])\n",
        "print(train_data[:WINDOW_SIZE * 2])\n",
        "print(train_data[0])\n",
        "len(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            "\n",
            "Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            "\n",
            "[(Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            "), (Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 2030\n",
            "[torch.LongTensor of size 1x1]\n",
            "), (Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 1227\n",
            "[torch.LongTensor of size 1x1]\n",
            "), (Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            "), (Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 2030\n",
            "[torch.LongTensor of size 1x1]\n",
            "), (Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 1227\n",
            "[torch.LongTensor of size 1x1]\n",
            ")]\n",
            "(Variable containing:\n",
            " 1284\n",
            "[torch.LongTensor of size 1x1]\n",
            ", Variable containing:\n",
            " 26\n",
            "[torch.LongTensor of size 1x1]\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "_syVBYDf4hgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Skipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, projection_dim):\n",
        "        super(Skipgram, self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
        "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
        "        \n",
        "        # Weight Initialization\n",
        "        self.embedding_v.weight.data.uniform_(-1, 1) \n",
        "        self.embedding_u.weight.data.uniform_(0, 0)\n",
        "        \n",
        "    def forward(self, center_words, target_words, outer_words):\n",
        "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
        "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
        "        outer_embeds = self.embedding_u(outer_words) # B x V x D\n",
        "        \n",
        "        # Bx1xD * BxDx1 = Bx1 (Batch Matrix Multiplication)\n",
        "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        \n",
        "        # BxVxD * BxDx1 = BxV (Batch Matrix Multiplication)\n",
        "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        \n",
        "        # Negative Log Likelihood (Log Softmax)\n",
        "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
        "        \n",
        "        return nll\n",
        "    \n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "        \n",
        "        return embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGR3Rc7Q4hg2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "id": "SyIoUGYq4hg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 30\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 100\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "losses = []\n",
        "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
        "\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6xS_u8E4hg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c3b0187-4ce3-44d9-f09f-0d9f162d534e"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCH):\n",
        "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "        inputs, targets = zip(*batch)\n",
        "        \n",
        "        inputs = torch.cat(inputs) # Bx1\n",
        "        targets = torch.cat(targets) # Bx1\n",
        "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab)) # BxV\n",
        "        loss = model(inputs, targets, vocabs)\n",
        "        \n",
        "        # Backpropagation\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data[0])\n",
        "        \n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch : %d, Mean_Loss : %.02f\" % (epoch, np.mean(losses)))\n",
        "        losses = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0, Mean_Loss : 6.33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TLcqi_Kn4hhD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "PKKV1pFS4hhE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_similarity(target, vocab):  \n",
        "    target_V = model.prediction(prepare_word(target, word2index))\n",
        "    similarities = []\n",
        "    \n",
        "    for i in range(len(vocab)):\n",
        "        if vocab[i] == target:\n",
        "            continue\n",
        "        \n",
        "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
        "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
        "        similarities.append([vocab[i], cosine_sim])\n",
        "    # Sort by similarity (Top 10 return)\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGZJObQy4hhJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5efa64b-0975-41e0-c122-42b9e13e495d"
      },
      "cell_type": "code",
      "source": [
        "test = random.choice(list(vocab))\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'butt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "Pq9942eT4hhN",
        "colab_type": "code",
        "colab": {},
        "outputId": "f46c82f6-ac24-4e0f-d3ca-cc38669bbb60"
      },
      "cell_type": "code",
      "source": [
        "word_similarity(test, vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['wharton', 0.6740432977676392],\n",
              " ['bowes', 0.6615484356880188],\n",
              " ['cuvier', 0.6574292182922363],\n",
              " ['desks', 0.6492806077003479],\n",
              " ['salted', 0.6338767409324646],\n",
              " ['elizabeth', 0.6323466300964355],\n",
              " ['turn', 0.6268795728683472],\n",
              " [').', 0.6265820860862732],\n",
              " ['dreary', 0.6198908686637878],\n",
              " ['sharks', 0.6184421181678772]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "BgSeNlma4hhS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}